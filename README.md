# Lip_sync-

This repository contains a **complete pipeline for high-quality lip-syncing** of videos using **LatentSync** combined with **Whisper-based audio encoding**. The system generates realistic, synced mouth movements for a given speaker video using external audio input.

---

## ğŸ“ **Project Description**

**LatentSync** is an advanced framework for generating accurate lip-sync in videos. Unlike basic frame-to-frame sync methods, **LatentSync works in the *latent feature space*** for better temporal consistency and realistic articulation.

This project allows you to:

* Upload any **video** of a person speaking
* Provide **custom external audio**
* Generate a **synchronized video output** where lip movements match the new audio
* Leverage **Whisper tiny model** for audio feature extraction
* Use **LatentSync UNet model** for generating the synced video

---

## ğŸ“‚ **Folder Structure**

```
LatentSync/
â”œâ”€â”€ Lip_sync.ipynb         # Main notebook for running inference
â”œâ”€â”€ README.md              # Project description and setup instructions
â”œâ”€â”€ checkpoints/           # Pre-trained models (e.g., latentsync_unet.pt, tiny.pt)
â”‚   â”œâ”€â”€ latentsync_unet.pt
â”‚   â””â”€â”€ whisper/
â”‚       â””â”€â”€ tiny.pt        # Whisper ASR model (required)
â”œâ”€â”€ configs/               # Model configuration YAML files
â”œâ”€â”€ scripts/               # Python scripts for inference
â”‚   â””â”€â”€ inference.py       # Main inference script
â”œâ”€â”€ latentsync/            # LatentSync source code
â”‚   â””â”€â”€ whisper/           # Whisper codebase (ASR feature extraction)
â”œâ”€â”€ FirstVideo.mp4         # Example input video (replace with your own)
â”œâ”€â”€ audio11.wav            # Example input audio (replace with your own)
â””â”€â”€ result/                # Generated output video (result.mp4)


```

---

### ğŸ“‚ Requirements

1. **LatentSync UNet Checkpoint**

2. **Whisper Tiny Checkpoint**

---



## âš™ï¸ **How to Run**

### âœ… **Step 1: Clone Repository**

```bash
git clone https://github.com/your-username/your-repo-name.git
```

### âœ… **Step 2: Install Requirements (Already configured in Colab)**

```bash
pip install -r requirements.txt
```

### âœ… **Step 3: Upload Inputs**

* Upload your **video (.mp4)** and **audio (.wav)** files.

### âœ… **Step 4: Run Inference**

```bash
!python -m scripts.inference \
  --unet_config_path configs/unet/stage2.yaml \
  --inference_ckpt_path checkpoints/latentsync_unet.pt \
  --video_path /path/to/your/video.mp4 \
  --audio_path /path/to/your/audio.wav \
  --video_out_path /path/to/output/result.mp4
```

---


## âš ï¸ **Notes**

* **Video Length:** Long videos may take **30+ minutes**. For quick testing, use **short 5-10 second clips**.
* **Face Restoration:** After inference, faces are automatically restored for better visual quality.

---

## ğŸ¤ **Contributing**

Pull requests and feature requests are welcome.

---

